오늘 수업 내용 : 지난 2주 간 내용 리뷰
1.딥러닝은 왜 딥러닝일까? = 일종의 리브랜딩이다. 기존 신경망은 대중적으로 실망을 안겨줬다. 그래서 리브랜딩. 신경망에서 두 단위 넘어가는 것이 "깊은 신경망"=DNN이다. 즉 MLP에서 히든레이어가 2개 이상인 것
-> 신경망이 깊어질수록 시그모이드 활성화 함수는 기울기 소실 문제가 발생함

그러던 중 렐루 함수의 등장으로 더 깊어진 신경망으로 학습이 가능해짐.
지도학습 알고리즘(머신러닝)
k-최근접 이웃 (k-Nearest Neighbor)
나이브 베이즈 (Naive Bayes)
의사결정 트리 (Decision Trees)
분류 규칙 학습자 (Classification Rule Learners)
선형 회귀 (Linear Regression)
회귀 트리 (Regression Trees)
모델 트리 (Model Trees)
신경망 (Neural Networks) -> 딥러닝 : 지금 주류
서포트 벡터 머신 (SVM:Support Vector Machines) : 10년 전까지는 주류였음
퍼셉트론은 학습이 불가하고 MLP(전파 역전파)만 가능하다.

배치사이즈 : 한 번에 6만개 학습이 어려우니 600개짜리로 100번 학습을 시키면 동일한 효과 기대

functional API는 입출력을 기준으로 하는 거 = 파이토치

Tensor란? = 행렬, 몇 차원 행렬이냐?. 행렬의 차원을 rank라 부른다.

dtype (28,28,1에 6만)??
러닝레이트는 학습 간격을 의미한다.
샘플링 : 예를 들어 오디오를 아날로그를 디지탈값으로 바꿔야하는데 이과정에 샘플링이 들어감

이항분류/ 다항분류

엄밀하게 다항분류는 없다. 이항분류만 각각 적용하는 것이다. 합을 정규화해서 1로 만들고 제일 큰 값만 남기고 나머지 버리기.

10.엔트로피란? : 열역학에서는 엔트로피 증가 법칙, 컴퓨터공학에서는 엔트로피부호화 압축이 잘 되는가??

크로스엔트로피는? : 집단 간 우수한 기준이 있을꺼 아님? 평균값, 최소값 등 기준이 되는게 크로스엔트로피.
통계 두 집단 간의 분포차이

0하고 이상한 6, 9를 어떻게 비교해?
6하고 0하고 정규분포 간의 비교 = 떨어져있으면 구별 쉬움 , 제일 가까운쪽을 선택하겠습니다.

원핫인코딩 : 카테고리칼크로스엔트로피랑 비슷한 것임
sparse하다? : 값보다 0이 많다 -> 숫자 많은거만 처리하다.
mnist같이 0이 많으면 손실함수를 sparse로 지정.
cifar10(칼라)는 반대로.

11. 학습정확도 vs 테스트정확도

에포크 증가하면 학습 정확도향상되지만 테스트 정확도는 별차이 없다(미미하다)

과적합이 심화됨.

과적합 방지기술
드랍아웃, 드랍커넥트
노드 수, 레이어 수 줄이기
Regularization
Norm 계산 :

출력값(토익점수)이 제대로 안 나왔다, -> 가중치 조정해서 더 나와야겠네?

Ex)

천만원 모금해야돼 -> 만원씩 천명 방법, 부자한테 한 명 천만원
작은 돈 모으는 방법, 큰 돈 모으는 방법이랑 결과는 같다.?

실제 MLP에서는 학습이 되었다는 전제 하에서.!
바람직한 건 골고루 분산되는게 좋은데 가중치가 골고루 분산되지 않는다. 특정한 가중치 노드,엣지에 의존하는 경우가 많다(이것도 과적합으로).그래서 레귤러라이제이션을 하는 이유이다.

L1 : 절대값에 비례하는 비용을 적용 -> 경마에 잘 뛰는 말은 모래 주머니 더 무거운 거 달기
(비레세)
L2 : 절대값의 제곱에 비례하는 비용을 적용 -> 누진세 개념, 5천만원 벌면 100만원 1억벌면 2백
(누진세)

전반적으로 가중치가 골고루된다, 과적합을 피한다.

13. 가중치 초기값 최적화

배그를 예로 들면 자기장 복불복이 좋음?--> 아님 그래서 초기값 설정이 중요하단거

최종학습치랑 처음 랜덤학습치랑 다르면 많이 조정해야하니까

초기값(난수) 설정에 최적화하는게 가중치 초기값 최적화이다.