{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "661b6901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a37609",
   "metadata": {},
   "source": [
    "# 파이토치로 선형 회귀 구현하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d47302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2400 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch   10/2400 W: 1.224, b: 0.516 Cost: 1.844294\n",
      "Epoch   20/2400 W: 1.550, b: 0.638 Cost: 0.239337\n",
      "Epoch   30/2400 W: 1.655, b: 0.665 Cost: 0.083519\n",
      "Epoch   40/2400 W: 1.693, b: 0.662 Cost: 0.065829\n",
      "Epoch   50/2400 W: 1.709, b: 0.650 Cost: 0.061424\n",
      "Epoch   60/2400 W: 1.719, b: 0.636 Cost: 0.058413\n",
      "Epoch   70/2400 W: 1.726, b: 0.621 Cost: 0.055656\n",
      "Epoch   80/2400 W: 1.733, b: 0.607 Cost: 0.053039\n",
      "Epoch   90/2400 W: 1.739, b: 0.592 Cost: 0.050546\n",
      "Epoch  100/2400 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  110/2400 W: 1.752, b: 0.564 Cost: 0.045907\n",
      "Epoch  120/2400 W: 1.758, b: 0.551 Cost: 0.043749\n",
      "Epoch  130/2400 W: 1.763, b: 0.538 Cost: 0.041693\n",
      "Epoch  140/2400 W: 1.769, b: 0.525 Cost: 0.039734\n",
      "Epoch  150/2400 W: 1.775, b: 0.513 Cost: 0.037866\n",
      "Epoch  160/2400 W: 1.780, b: 0.500 Cost: 0.036087\n",
      "Epoch  170/2400 W: 1.785, b: 0.488 Cost: 0.034391\n",
      "Epoch  180/2400 W: 1.790, b: 0.477 Cost: 0.032775\n",
      "Epoch  190/2400 W: 1.795, b: 0.465 Cost: 0.031234\n",
      "Epoch  200/2400 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  210/2400 W: 1.805, b: 0.444 Cost: 0.028368\n",
      "Epoch  220/2400 W: 1.809, b: 0.433 Cost: 0.027034\n",
      "Epoch  230/2400 W: 1.814, b: 0.423 Cost: 0.025764\n",
      "Epoch  240/2400 W: 1.818, b: 0.413 Cost: 0.024553\n",
      "Epoch  250/2400 W: 1.823, b: 0.403 Cost: 0.023399\n",
      "Epoch  260/2400 W: 1.827, b: 0.393 Cost: 0.022299\n",
      "Epoch  270/2400 W: 1.831, b: 0.384 Cost: 0.021252\n",
      "Epoch  280/2400 W: 1.835, b: 0.375 Cost: 0.020253\n",
      "Epoch  290/2400 W: 1.839, b: 0.366 Cost: 0.019301\n",
      "Epoch  300/2400 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  310/2400 W: 1.847, b: 0.349 Cost: 0.017529\n",
      "Epoch  320/2400 W: 1.850, b: 0.340 Cost: 0.016706\n",
      "Epoch  330/2400 W: 1.854, b: 0.332 Cost: 0.015921\n",
      "Epoch  340/2400 W: 1.857, b: 0.324 Cost: 0.015172\n",
      "Epoch  350/2400 W: 1.861, b: 0.317 Cost: 0.014459\n",
      "Epoch  360/2400 W: 1.864, b: 0.309 Cost: 0.013780\n",
      "Epoch  370/2400 W: 1.867, b: 0.302 Cost: 0.013132\n",
      "Epoch  380/2400 W: 1.870, b: 0.295 Cost: 0.012515\n",
      "Epoch  390/2400 W: 1.873, b: 0.288 Cost: 0.011927\n",
      "Epoch  400/2400 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  410/2400 W: 1.879, b: 0.274 Cost: 0.010832\n",
      "Epoch  420/2400 W: 1.882, b: 0.268 Cost: 0.010323\n",
      "Epoch  430/2400 W: 1.885, b: 0.261 Cost: 0.009838\n",
      "Epoch  440/2400 W: 1.888, b: 0.255 Cost: 0.009376\n",
      "Epoch  450/2400 W: 1.890, b: 0.249 Cost: 0.008935\n",
      "Epoch  460/2400 W: 1.893, b: 0.243 Cost: 0.008515\n",
      "Epoch  470/2400 W: 1.896, b: 0.237 Cost: 0.008115\n",
      "Epoch  480/2400 W: 1.898, b: 0.232 Cost: 0.007733\n",
      "Epoch  490/2400 W: 1.901, b: 0.226 Cost: 0.007370\n",
      "Epoch  500/2400 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  510/2400 W: 1.905, b: 0.215 Cost: 0.006694\n",
      "Epoch  520/2400 W: 1.907, b: 0.210 Cost: 0.006379\n",
      "Epoch  530/2400 W: 1.910, b: 0.205 Cost: 0.006079\n",
      "Epoch  540/2400 W: 1.912, b: 0.200 Cost: 0.005794\n",
      "Epoch  550/2400 W: 1.914, b: 0.196 Cost: 0.005521\n",
      "Epoch  560/2400 W: 1.916, b: 0.191 Cost: 0.005262\n",
      "Epoch  570/2400 W: 1.918, b: 0.187 Cost: 0.005014\n",
      "Epoch  580/2400 W: 1.920, b: 0.182 Cost: 0.004779\n",
      "Epoch  590/2400 W: 1.922, b: 0.178 Cost: 0.004554\n",
      "Epoch  600/2400 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  610/2400 W: 1.925, b: 0.169 Cost: 0.004136\n",
      "Epoch  620/2400 W: 1.927, b: 0.165 Cost: 0.003942\n",
      "Epoch  630/2400 W: 1.929, b: 0.161 Cost: 0.003757\n",
      "Epoch  640/2400 W: 1.931, b: 0.158 Cost: 0.003580\n",
      "Epoch  650/2400 W: 1.932, b: 0.154 Cost: 0.003412\n",
      "Epoch  660/2400 W: 1.934, b: 0.150 Cost: 0.003251\n",
      "Epoch  670/2400 W: 1.936, b: 0.147 Cost: 0.003099\n",
      "Epoch  680/2400 W: 1.937, b: 0.143 Cost: 0.002953\n",
      "Epoch  690/2400 W: 1.939, b: 0.140 Cost: 0.002814\n",
      "Epoch  700/2400 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  710/2400 W: 1.941, b: 0.133 Cost: 0.002556\n",
      "Epoch  720/2400 W: 1.943, b: 0.130 Cost: 0.002436\n",
      "Epoch  730/2400 W: 1.944, b: 0.127 Cost: 0.002321\n",
      "Epoch  740/2400 W: 1.946, b: 0.124 Cost: 0.002212\n",
      "Epoch  750/2400 W: 1.947, b: 0.121 Cost: 0.002108\n",
      "Epoch  760/2400 W: 1.948, b: 0.118 Cost: 0.002009\n",
      "Epoch  770/2400 W: 1.949, b: 0.115 Cost: 0.001915\n",
      "Epoch  780/2400 W: 1.951, b: 0.113 Cost: 0.001825\n",
      "Epoch  790/2400 W: 1.952, b: 0.110 Cost: 0.001739\n",
      "Epoch  800/2400 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  810/2400 W: 1.954, b: 0.105 Cost: 0.001579\n",
      "Epoch  820/2400 W: 1.955, b: 0.102 Cost: 0.001505\n",
      "Epoch  830/2400 W: 1.956, b: 0.100 Cost: 0.001434\n",
      "Epoch  840/2400 W: 1.957, b: 0.097 Cost: 0.001367\n",
      "Epoch  850/2400 W: 1.958, b: 0.095 Cost: 0.001303\n",
      "Epoch  860/2400 W: 1.959, b: 0.093 Cost: 0.001242\n",
      "Epoch  870/2400 W: 1.960, b: 0.091 Cost: 0.001183\n",
      "Epoch  880/2400 W: 1.961, b: 0.088 Cost: 0.001128\n",
      "Epoch  890/2400 W: 1.962, b: 0.086 Cost: 0.001075\n",
      "Epoch  900/2400 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch  910/2400 W: 1.964, b: 0.082 Cost: 0.000976\n",
      "Epoch  920/2400 W: 1.965, b: 0.080 Cost: 0.000930\n",
      "Epoch  930/2400 W: 1.966, b: 0.078 Cost: 0.000886\n",
      "Epoch  940/2400 W: 1.966, b: 0.077 Cost: 0.000845\n",
      "Epoch  950/2400 W: 1.967, b: 0.075 Cost: 0.000805\n",
      "Epoch  960/2400 W: 1.968, b: 0.073 Cost: 0.000767\n",
      "Epoch  970/2400 W: 1.969, b: 0.071 Cost: 0.000731\n",
      "Epoch  980/2400 W: 1.969, b: 0.070 Cost: 0.000697\n",
      "Epoch  990/2400 W: 1.970, b: 0.068 Cost: 0.000664\n",
      "Epoch 1000/2400 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1010/2400 W: 1.972, b: 0.065 Cost: 0.000603\n",
      "Epoch 1020/2400 W: 1.972, b: 0.063 Cost: 0.000575\n",
      "Epoch 1030/2400 W: 1.973, b: 0.062 Cost: 0.000548\n",
      "Epoch 1040/2400 W: 1.974, b: 0.060 Cost: 0.000522\n",
      "Epoch 1050/2400 W: 1.974, b: 0.059 Cost: 0.000497\n",
      "Epoch 1060/2400 W: 1.975, b: 0.057 Cost: 0.000474\n",
      "Epoch 1070/2400 W: 1.975, b: 0.056 Cost: 0.000452\n",
      "Epoch 1080/2400 W: 1.976, b: 0.055 Cost: 0.000431\n",
      "Epoch 1090/2400 W: 1.977, b: 0.053 Cost: 0.000410\n",
      "Epoch 1100/2400 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1110/2400 W: 1.978, b: 0.051 Cost: 0.000373\n",
      "Epoch 1120/2400 W: 1.978, b: 0.050 Cost: 0.000355\n",
      "Epoch 1130/2400 W: 1.979, b: 0.048 Cost: 0.000338\n",
      "Epoch 1140/2400 W: 1.979, b: 0.047 Cost: 0.000323\n",
      "Epoch 1150/2400 W: 1.980, b: 0.046 Cost: 0.000307\n",
      "Epoch 1160/2400 W: 1.980, b: 0.045 Cost: 0.000293\n",
      "Epoch 1170/2400 W: 1.981, b: 0.044 Cost: 0.000279\n",
      "Epoch 1180/2400 W: 1.981, b: 0.043 Cost: 0.000266\n",
      "Epoch 1190/2400 W: 1.982, b: 0.042 Cost: 0.000254\n",
      "Epoch 1200/2400 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1210/2400 W: 1.982, b: 0.040 Cost: 0.000230\n",
      "Epoch 1220/2400 W: 1.983, b: 0.039 Cost: 0.000219\n",
      "Epoch 1230/2400 W: 1.983, b: 0.038 Cost: 0.000209\n",
      "Epoch 1240/2400 W: 1.984, b: 0.037 Cost: 0.000199\n",
      "Epoch 1250/2400 W: 1.984, b: 0.036 Cost: 0.000190\n",
      "Epoch 1260/2400 W: 1.984, b: 0.035 Cost: 0.000181\n",
      "Epoch 1270/2400 W: 1.985, b: 0.035 Cost: 0.000173\n",
      "Epoch 1280/2400 W: 1.985, b: 0.034 Cost: 0.000164\n",
      "Epoch 1290/2400 W: 1.985, b: 0.033 Cost: 0.000157\n",
      "Epoch 1300/2400 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1310/2400 W: 1.986, b: 0.031 Cost: 0.000142\n",
      "Epoch 1320/2400 W: 1.987, b: 0.031 Cost: 0.000136\n",
      "Epoch 1330/2400 W: 1.987, b: 0.030 Cost: 0.000129\n",
      "Epoch 1340/2400 W: 1.987, b: 0.029 Cost: 0.000123\n",
      "Epoch 1350/2400 W: 1.987, b: 0.029 Cost: 0.000117\n",
      "Epoch 1360/2400 W: 1.988, b: 0.028 Cost: 0.000112\n",
      "Epoch 1370/2400 W: 1.988, b: 0.027 Cost: 0.000107\n",
      "Epoch 1380/2400 W: 1.988, b: 0.027 Cost: 0.000102\n",
      "Epoch 1390/2400 W: 1.989, b: 0.026 Cost: 0.000097\n",
      "Epoch 1400/2400 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1410/2400 W: 1.989, b: 0.025 Cost: 0.000088\n",
      "Epoch 1420/2400 W: 1.989, b: 0.024 Cost: 0.000084\n",
      "Epoch 1430/2400 W: 1.990, b: 0.024 Cost: 0.000080\n",
      "Epoch 1440/2400 W: 1.990, b: 0.023 Cost: 0.000076\n",
      "Epoch 1450/2400 W: 1.990, b: 0.022 Cost: 0.000073\n",
      "Epoch 1460/2400 W: 1.990, b: 0.022 Cost: 0.000069\n",
      "Epoch 1470/2400 W: 1.991, b: 0.021 Cost: 0.000066\n",
      "Epoch 1480/2400 W: 1.991, b: 0.021 Cost: 0.000063\n",
      "Epoch 1490/2400 W: 1.991, b: 0.020 Cost: 0.000060\n",
      "Epoch 1500/2400 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1510/2400 W: 1.991, b: 0.019 Cost: 0.000054\n",
      "Epoch 1520/2400 W: 1.992, b: 0.019 Cost: 0.000052\n",
      "Epoch 1530/2400 W: 1.992, b: 0.019 Cost: 0.000049\n",
      "Epoch 1540/2400 W: 1.992, b: 0.018 Cost: 0.000047\n",
      "Epoch 1550/2400 W: 1.992, b: 0.018 Cost: 0.000045\n",
      "Epoch 1560/2400 W: 1.992, b: 0.017 Cost: 0.000043\n",
      "Epoch 1570/2400 W: 1.993, b: 0.017 Cost: 0.000041\n",
      "Epoch 1580/2400 W: 1.993, b: 0.016 Cost: 0.000039\n",
      "Epoch 1590/2400 W: 1.993, b: 0.016 Cost: 0.000037\n",
      "Epoch 1600/2400 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1610/2400 W: 1.993, b: 0.015 Cost: 0.000034\n",
      "Epoch 1620/2400 W: 1.993, b: 0.015 Cost: 0.000032\n",
      "Epoch 1630/2400 W: 1.994, b: 0.015 Cost: 0.000030\n",
      "Epoch 1640/2400 W: 1.994, b: 0.014 Cost: 0.000029\n",
      "Epoch 1650/2400 W: 1.994, b: 0.014 Cost: 0.000028\n",
      "Epoch 1660/2400 W: 1.994, b: 0.014 Cost: 0.000026\n",
      "Epoch 1670/2400 W: 1.994, b: 0.013 Cost: 0.000025\n",
      "Epoch 1680/2400 W: 1.994, b: 0.013 Cost: 0.000024\n",
      "Epoch 1690/2400 W: 1.994, b: 0.013 Cost: 0.000023\n",
      "Epoch 1700/2400 W: 1.995, b: 0.012 Cost: 0.000022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1710/2400 W: 1.995, b: 0.012 Cost: 0.000021\n",
      "Epoch 1720/2400 W: 1.995, b: 0.012 Cost: 0.000020\n",
      "Epoch 1730/2400 W: 1.995, b: 0.011 Cost: 0.000019\n",
      "Epoch 1740/2400 W: 1.995, b: 0.011 Cost: 0.000018\n",
      "Epoch 1750/2400 W: 1.995, b: 0.011 Cost: 0.000017\n",
      "Epoch 1760/2400 W: 1.995, b: 0.011 Cost: 0.000016\n",
      "Epoch 1770/2400 W: 1.995, b: 0.010 Cost: 0.000016\n",
      "Epoch 1780/2400 W: 1.996, b: 0.010 Cost: 0.000015\n",
      "Epoch 1790/2400 W: 1.996, b: 0.010 Cost: 0.000014\n",
      "Epoch 1800/2400 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1810/2400 W: 1.996, b: 0.009 Cost: 0.000013\n",
      "Epoch 1820/2400 W: 1.996, b: 0.009 Cost: 0.000012\n",
      "Epoch 1830/2400 W: 1.996, b: 0.009 Cost: 0.000012\n",
      "Epoch 1840/2400 W: 1.996, b: 0.009 Cost: 0.000011\n",
      "Epoch 1850/2400 W: 1.996, b: 0.009 Cost: 0.000011\n",
      "Epoch 1860/2400 W: 1.996, b: 0.008 Cost: 0.000010\n",
      "Epoch 1870/2400 W: 1.996, b: 0.008 Cost: 0.000010\n",
      "Epoch 1880/2400 W: 1.996, b: 0.008 Cost: 0.000009\n",
      "Epoch 1890/2400 W: 1.997, b: 0.008 Cost: 0.000009\n",
      "Epoch 1900/2400 W: 1.997, b: 0.008 Cost: 0.000008\n",
      "Epoch 1910/2400 W: 1.997, b: 0.007 Cost: 0.000008\n",
      "Epoch 1920/2400 W: 1.997, b: 0.007 Cost: 0.000008\n",
      "Epoch 1930/2400 W: 1.997, b: 0.007 Cost: 0.000007\n",
      "Epoch 1940/2400 W: 1.997, b: 0.007 Cost: 0.000007\n",
      "Epoch 1950/2400 W: 1.997, b: 0.007 Cost: 0.000007\n",
      "Epoch 1960/2400 W: 1.997, b: 0.007 Cost: 0.000006\n",
      "Epoch 1970/2400 W: 1.997, b: 0.006 Cost: 0.000006\n",
      "Epoch 1980/2400 W: 1.997, b: 0.006 Cost: 0.000006\n",
      "Epoch 1990/2400 W: 1.997, b: 0.006 Cost: 0.000005\n",
      "Epoch 2000/2400 W: 1.997, b: 0.006 Cost: 0.000005\n",
      "Epoch 2010/2400 W: 1.997, b: 0.006 Cost: 0.000005\n",
      "Epoch 2020/2400 W: 1.997, b: 0.006 Cost: 0.000005\n",
      "Epoch 2030/2400 W: 1.998, b: 0.006 Cost: 0.000004\n",
      "Epoch 2040/2400 W: 1.998, b: 0.005 Cost: 0.000004\n",
      "Epoch 2050/2400 W: 1.998, b: 0.005 Cost: 0.000004\n",
      "Epoch 2060/2400 W: 1.998, b: 0.005 Cost: 0.000004\n",
      "Epoch 2070/2400 W: 1.998, b: 0.005 Cost: 0.000004\n",
      "Epoch 2080/2400 W: 1.998, b: 0.005 Cost: 0.000003\n",
      "Epoch 2090/2400 W: 1.998, b: 0.005 Cost: 0.000003\n",
      "Epoch 2100/2400 W: 1.998, b: 0.005 Cost: 0.000003\n",
      "Epoch 2110/2400 W: 1.998, b: 0.005 Cost: 0.000003\n",
      "Epoch 2120/2400 W: 1.998, b: 0.004 Cost: 0.000003\n",
      "Epoch 2130/2400 W: 1.998, b: 0.004 Cost: 0.000003\n",
      "Epoch 2140/2400 W: 1.998, b: 0.004 Cost: 0.000003\n",
      "Epoch 2150/2400 W: 1.998, b: 0.004 Cost: 0.000002\n",
      "Epoch 2160/2400 W: 1.998, b: 0.004 Cost: 0.000002\n",
      "Epoch 2170/2400 W: 1.998, b: 0.004 Cost: 0.000002\n",
      "Epoch 2180/2400 W: 1.998, b: 0.004 Cost: 0.000002\n",
      "Epoch 2190/2400 W: 1.998, b: 0.004 Cost: 0.000002\n",
      "Epoch 2200/2400 W: 1.998, b: 0.004 Cost: 0.000002\n",
      "Epoch 2210/2400 W: 1.998, b: 0.004 Cost: 0.000002\n",
      "Epoch 2220/2400 W: 1.998, b: 0.004 Cost: 0.000002\n",
      "Epoch 2230/2400 W: 1.998, b: 0.003 Cost: 0.000002\n",
      "Epoch 2240/2400 W: 1.999, b: 0.003 Cost: 0.000002\n",
      "Epoch 2250/2400 W: 1.999, b: 0.003 Cost: 0.000002\n",
      "Epoch 2260/2400 W: 1.999, b: 0.003 Cost: 0.000001\n",
      "Epoch 2270/2400 W: 1.999, b: 0.003 Cost: 0.000001\n",
      "Epoch 2280/2400 W: 1.999, b: 0.003 Cost: 0.000001\n",
      "Epoch 2290/2400 W: 1.999, b: 0.003 Cost: 0.000001\n",
      "Epoch 2300/2400 W: 1.999, b: 0.003 Cost: 0.000001\n",
      "Epoch 2310/2400 W: 1.999, b: 0.003 Cost: 0.000001\n",
      "Epoch 2320/2400 W: 1.999, b: 0.003 Cost: 0.000001\n",
      "Epoch 2330/2400 W: 1.999, b: 0.003 Cost: 0.000001\n",
      "Epoch 2340/2400 W: 1.999, b: 0.003 Cost: 0.000001\n",
      "Epoch 2350/2400 W: 1.999, b: 0.003 Cost: 0.000001\n",
      "Epoch 2360/2400 W: 1.999, b: 0.003 Cost: 0.000001\n",
      "Epoch 2370/2400 W: 1.999, b: 0.002 Cost: 0.000001\n",
      "Epoch 2380/2400 W: 1.999, b: 0.002 Cost: 0.000001\n",
      "Epoch 2390/2400 W: 1.999, b: 0.002 Cost: 0.000001\n",
      "Epoch 2400/2400 W: 1.999, b: 0.002 Cost: 0.000001\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 2400 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b5e9a2",
   "metadata": {},
   "source": [
    "   # optimizer.zero_grad()가 필요한 이유\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f88acbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 2.0\n",
      "수식을 w로 미분한 값 : 4.0\n",
      "수식을 w로 미분한 값 : 6.0\n",
      "수식을 w로 미분한 값 : 8.0\n",
      "수식을 w로 미분한 값 : 10.0\n",
      "수식을 w로 미분한 값 : 12.0\n",
      "수식을 w로 미분한 값 : 14.0\n",
      "수식을 w로 미분한 값 : 16.0\n",
      "수식을 w로 미분한 값 : 18.0\n",
      "수식을 w로 미분한 값 : 20.0\n",
      "수식을 w로 미분한 값 : 22.0\n",
      "수식을 w로 미분한 값 : 24.0\n",
      "수식을 w로 미분한 값 : 26.0\n",
      "수식을 w로 미분한 값 : 28.0\n",
      "수식을 w로 미분한 값 : 30.0\n",
      "수식을 w로 미분한 값 : 32.0\n",
      "수식을 w로 미분한 값 : 34.0\n",
      "수식을 w로 미분한 값 : 36.0\n",
      "수식을 w로 미분한 값 : 38.0\n",
      "수식을 w로 미분한 값 : 40.0\n",
      "수식을 w로 미분한 값 : 42.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    z = 2*w\n",
    "\n",
    "    z.backward()\n",
    "    print('수식을 w로 미분한 값 : {}'.format(w.grad))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd4583",
   "metadata": {},
   "source": [
    "# Autograd 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e7a50",
   "metadata": {},
   "source": [
    "값이 2인 임의의 스칼라 텐서 w를 선언. 이때 required_grad를 True로 설정=텐서에 대한 기울기를 저장\n",
    "w.grad에 w에 대한 미분값이 저장하는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44965ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb05eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수식정리\n",
    "\n",
    "y = w**2\n",
    "z = 2*y + 5\n",
    "\n",
    "# 해당 수식을 w에 대해서 미분. .backward()를 호출하면 해당 수식의 w에 대한 기울기를 자동으로 계산한다/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28aa3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71a1cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "source": [
    "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71720c4f",
   "metadata": {},
   "source": [
    "# nn module로 구현하는 선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c50b2456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# model = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "# cost = F.mse_loss(prediction, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21fea01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 선언\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d16a7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 선언 및 초기화. 단순 선형 회귀이므로 input_dim=1, output_dim=1.\n",
    "model = nn.Linear(1,1)\n",
    "\n",
    "#model에는 가중치 W와 편향 b가 이미 저장되어 있다.. 이 값은 model.parameters() 함수를 써서 불로온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62a438c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.3117]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.6683], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))\n",
    "\n",
    "# 첫번째 값이 W고, 두번째 값이 b. 두 값 모두 현재는 랜덤 초기화 t상태임 \n",
    "# 그리고 두 값 모두 학습의 대상이므로 requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "623bf71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저를 정의 model.parameters()를 사용하여 W와 b를 전달 가능 학습률은 0.01\n",
    "\n",
    "# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06a0887b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 31.565584\n",
      "Epoch  100/2000 Cost: 0.007565\n",
      "Epoch  200/2000 Cost: 0.004675\n",
      "Epoch  300/2000 Cost: 0.002889\n",
      "Epoch  400/2000 Cost: 0.001785\n",
      "Epoch  500/2000 Cost: 0.001103\n",
      "Epoch  600/2000 Cost: 0.000682\n",
      "Epoch  700/2000 Cost: 0.000421\n",
      "Epoch  800/2000 Cost: 0.000260\n",
      "Epoch  900/2000 Cost: 0.000161\n",
      "Epoch 1000/2000 Cost: 0.000099\n",
      "Epoch 1100/2000 Cost: 0.000061\n",
      "Epoch 1200/2000 Cost: 0.000038\n",
      "Epoch 1300/2000 Cost: 0.000023\n",
      "Epoch 1400/2000 Cost: 0.000014\n",
      "Epoch 1500/2000 Cost: 0.000009\n",
      "Epoch 1600/2000 Cost: 0.000006\n",
      "Epoch 1700/2000 Cost: 0.000003\n",
      "Epoch 1800/2000 Cost: 0.000002\n",
      "Epoch 1900/2000 Cost: 0.000001\n",
      "Epoch 2000/2000 Cost: 0.000001\n"
     ]
    }
   ],
   "source": [
    "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce417e",
   "metadata": {},
   "source": [
    "## - 학습 확인하기(전파하기, 출력값 확인해보기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7d6aa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 4일 때의 예측값 : tensor([[7.9982]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 4를 선언\n",
    "new_var =  torch.FloatTensor([[4.0]]) \n",
    "# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) # forward 연산\n",
    "# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n",
    "print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efd0101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y= 2x 가 정답이므로 y값이 8에 가까우면 W와 b의 값이 어느정도 최적화가 된 것임. \n",
    "#실제로 예측된 y값은 7.9989로 8에 매우 가깝다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7bafa",
   "metadata": {},
   "source": [
    "## - W , B 값 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8bac08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[1.9990]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0024], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f082df3",
   "metadata": {},
   "source": [
    "# 클래스로 구현하는 선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "76962604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module): # torch.nn.Module을 상속받는 파이썬 클래스\n",
    "    def __init__(self): #\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1) # 단순 선형 회귀이므로 input_dim=1, output_dim=1.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43e501af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b58bfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 12.491858\n",
      "Epoch  100/2000 Cost: 12.491858\n",
      "Epoch  200/2000 Cost: 12.491858\n",
      "Epoch  300/2000 Cost: 12.491858\n",
      "Epoch  400/2000 Cost: 12.491858\n",
      "Epoch  500/2000 Cost: 12.491858\n",
      "Epoch  600/2000 Cost: 12.491858\n",
      "Epoch  700/2000 Cost: 12.491858\n",
      "Epoch  800/2000 Cost: 12.491858\n",
      "Epoch  900/2000 Cost: 12.491858\n",
      "Epoch 1000/2000 Cost: 12.491858\n",
      "Epoch 1100/2000 Cost: 12.491858\n",
      "Epoch 1200/2000 Cost: 12.491858\n",
      "Epoch 1300/2000 Cost: 12.491858\n",
      "Epoch 1400/2000 Cost: 12.491858\n",
      "Epoch 1500/2000 Cost: 12.491858\n",
      "Epoch 1600/2000 Cost: 12.491858\n",
      "Epoch 1700/2000 Cost: 12.491858\n",
      "Epoch 1800/2000 Cost: 12.491858\n",
      "Epoch 1900/2000 Cost: 12.491858\n",
      "Epoch 2000/2000 Cost: 12.491858\n"
     ]
    }
   ],
   "source": [
    "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31683f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
